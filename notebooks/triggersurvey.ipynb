{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seasonal-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "from lpcjobqueue import LPCCondorCluster\n",
    "from distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/coffea/util.py:154: FutureWarning: In coffea version v2023.3.0 (target date: 31 Mar 2023), this will be an error.\n",
      "(Set coffea.deprecations_as_errors = True to get a stack trace now.)\n",
      "ImportError: coffea.hist is deprecated\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from coffea import processor\n",
    "import coffea.hist\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "\n",
    "\n",
    "def PackedSelection_any(self, *names):\n",
    "    consider = 0\n",
    "    for name in names:\n",
    "        idx = self._names.index(name)\n",
    "        consider |= 1 << idx\n",
    "    return (self._data & consider) != 0\n",
    "\n",
    "\n",
    "class TriggerProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, year=\"2017\"):\n",
    "        self._year = year\n",
    "        self._triggers = {\n",
    "            '2016': [\n",
    "                'PFHT800',\n",
    "                'PFHT900',\n",
    "                'AK8PFJet360_TrimMass30',\n",
    "                'AK8PFHT700_TrimR0p1PT0p03Mass50',\n",
    "                'PFHT650_WideJetMJJ950DEtaJJ1p5',\n",
    "                'PFHT650_WideJetMJJ900DEtaJJ1p5',\n",
    "                'AK8DiPFJet280_200_TrimMass30_BTagCSV_p20',\n",
    "                'PFJet450',\n",
    "            ],\n",
    "            '2017': [\n",
    "                'AK8PFJet330_PFAK8BTagCSV_p17',\n",
    "                'PFHT1050',\n",
    "                'AK8PFJet400_TrimMass30',\n",
    "                'AK8PFJet420_TrimMass30', # redundant\n",
    "                'AK8PFHT800_TrimMass50',\n",
    "                'PFJet500',\n",
    "                'AK8PFJet500',\n",
    "\n",
    "            ],\n",
    "            '2018': [\n",
    "                'AK8PFJet400_TrimMass30',\n",
    "                'AK8PFJet420_TrimMass30',\n",
    "                'AK8PFHT800_TrimMass50',\n",
    "                'PFHT1050',\n",
    "                'PFJet500',\n",
    "                'AK8PFJet500',\n",
    "                'AK8PFJet330_TrimMass30_PFAK8BoostedDoubleB_np4',\n",
    "            ],\n",
    "        }\n",
    "        # https://twiki.cern.ch/twiki/bin/viewauth/CMS/MuonHLT\n",
    "        self._mutriggers = {\n",
    "            '2016': [\n",
    "                \"IsoMu24\",\n",
    "                \"IsoTkMu24\",\n",
    "                \"Mu50\",\n",
    "                \"TkMu50\",\n",
    "            ],\n",
    "            '2017': [\n",
    "                \"IsoMu27\",\n",
    "                \"Mu50\",\n",
    "                \"OldMu100\",  # not in all eras\n",
    "                \"TkMu100\",\n",
    "            ],\n",
    "            '2018': [\n",
    "                \"IsoMu24\",\n",
    "                \"Mu50\",\n",
    "                \"OldMu100\",\n",
    "                \"TkMu100\",\n",
    "            ]\n",
    "        }\n",
    "        self._era_runranges = {\n",
    "            \"Run2016B\": (272007, 275376),\n",
    "            \"Run2016C\": (275657, 276283),\n",
    "            \"Run2016D\": (276315, 276811),\n",
    "            \"Run2016E\": (276831, 277420),\n",
    "            \"Run2016F\": (277772, 278808),\n",
    "            \"Run2016G\": (278820, 280385),\n",
    "            \"Run2016H\": (280919, 284044),\n",
    "            \"2017A\": (294645, 297019),\n",
    "            \"2017B\": (297020, 299329),\n",
    "            \"2017C\": (299337, 302029),\n",
    "            \"2017D\": (302030, 303434),\n",
    "            \"2017E\": (303435, 304826),\n",
    "            \"2017F\": (304911, 306462),\n",
    "            \"Run2018A\": (315252, 316995),\n",
    "            \"Run2018B\": (316998, 319312),\n",
    "            \"Run2018C\": (319313, 320393),\n",
    "            \"Run2018D\": (320394, 325273),\n",
    "            \"Run2018E\": (325274, 325765),\n",
    "        }\n",
    "        \n",
    "        commonaxes = (\n",
    "            coffea.hist.Cat(\"dataset\", \"Dataset name\"),\n",
    "            coffea.hist.Cat(\"era\", \"Run era\"),\n",
    "            coffea.hist.Bin(\"pt\", \"Leading jet $p_T$\", 100, 0, 1000),\n",
    "            coffea.hist.Bin(\"msd\", \"Leading jet $m_{SD}$\", 30, 0, 300),\n",
    "            coffea.hist.Bin(\"ddb\", \"Leading jet DDBvL score\", 20, 0, 1),\n",
    "        )\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            \"nevents\": processor.defaultdict_accumulator(float),\n",
    "            \"trigger_exclusive\": coffea.hist.Hist(\n",
    "                \"Events\",\n",
    "                coffea.hist.Cat(\"trigger\", \"Trigger name\"),\n",
    "                *commonaxes\n",
    "            ),\n",
    "            \"trigger_inclusive\": coffea.hist.Hist(\n",
    "                \"Events\",\n",
    "                coffea.hist.Cat(\"trigger\", \"Trigger name\"),\n",
    "                *commonaxes\n",
    "            ),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        isRealData = not \"genWeight\" in events.fields\n",
    "        if isRealData:\n",
    "            for name, (runlo, runhi) in self._era_runranges.items():\n",
    "                # assumes no era will be split across input files\n",
    "                if events.run[0] >= runlo and events.run[0] <= runhi:\n",
    "                    era = name\n",
    "                    break\n",
    "        else:\n",
    "            era = \"MC\"\n",
    "        output[\"nevents\"][dataset] += len(events)\n",
    "        \n",
    "        triggers = PackedSelection()\n",
    "        trigger_names = self._triggers[self._year]\n",
    "        for tname in trigger_names:\n",
    "            if tname in events.HLT.fields:\n",
    "                triggers.add(tname, events.HLT[tname])\n",
    "            else:\n",
    "                triggers.add(tname, np.zeros(len(events), dtype=bool))\n",
    "\n",
    "        # All with respect to independent muon reference trigger\n",
    "        muontrigger = np.zeros(len(events), dtype=bool)\n",
    "        for tname in self._mutriggers[self._year]:\n",
    "            if tname in events.HLT.fields:\n",
    "                muontrigger |= ak.to_numpy(events.HLT[tname])\n",
    "        muons = events.Muon[\n",
    "            (events.Muon.pt > 25)\n",
    "            & (abs(events.Muon.eta) < 2.4)\n",
    "            & (events.Muon.pfRelIso04_all < 0.25)\n",
    "            & events.Muon.looseId\n",
    "        ]\n",
    "        # take highest pT\n",
    "        jet = ak.firsts(events.FatJet[\n",
    "            (events.FatJet.pt > 200)\n",
    "            & (abs(events.FatJet.eta) < 2.5)\n",
    "            & events.FatJet.isTight\n",
    "            & ak.all(events.FatJet.metric_table(muons) > 0.8, axis=-1)  # default metric: delta_r\n",
    "        ])\n",
    "        jet_exists = ~ak.is_none(jet) & muontrigger\n",
    "\n",
    "        output[\"trigger_exclusive\"].fill(\n",
    "            dataset=dataset,\n",
    "            era=era,\n",
    "            pt=jet[jet_exists].pt,\n",
    "            msd=jet[jet_exists].msoftdrop,\n",
    "            ddb=jet[jet_exists].btagDDBvLV2,\n",
    "            trigger=\"none\",\n",
    "        )\n",
    "        cut = jet_exists & PackedSelection_any(triggers, *set(trigger_names))\n",
    "        output[\"trigger_inclusive\"].fill(\n",
    "            dataset=dataset,\n",
    "            era=era,\n",
    "            pt=jet[cut].pt,\n",
    "            msd=jet[cut].msoftdrop,\n",
    "            ddb=jet[cut].btagDDBvLV2,\n",
    "            trigger=\"all\",\n",
    "        )\n",
    "\n",
    "        for tname in trigger_names:\n",
    "            cut = jet_exists & triggers.all(tname)\n",
    "            output[\"trigger_exclusive\"].fill(\n",
    "                dataset=dataset,\n",
    "                era=era,\n",
    "                pt=jet[cut].pt,\n",
    "                msd=jet[cut].msoftdrop,\n",
    "                ddb=jet[cut].btagDDBvLV2,\n",
    "                trigger=tname,\n",
    "            )\n",
    "            cut = jet_exists & PackedSelection_any(triggers, *(set(trigger_names) - {tname}))\n",
    "            output[\"trigger_inclusive\"].fill(\n",
    "                dataset=dataset,\n",
    "                era=era,\n",
    "                pt=jet[cut].pt,\n",
    "                msd=jet[cut].msoftdrop,\n",
    "                ddb=jet[cut].btagDDBvLV2,\n",
    "                trigger=tname,\n",
    "            )\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-logic",
   "metadata": {},
   "source": [
    "# Test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "frequent-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nevents': defaultdict_accumulator(float,\n",
       "                         {'QCD_HT1000to1500_TuneCP5_PSWeights_13TeV-madgraph-pythia8': 100000.0}),\n",
       " 'trigger_exclusive': <Hist (trigger,dataset,era,pt,msd,ddb) instance at 0x7f8d1b2b1ee0>,\n",
       " 'trigger_inclusive': <Hist (trigger,dataset,era,pt,msd,ddb) instance at 0x7f8d1b2b1d30>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Found duplicate branch\")\n",
    "\n",
    "year = \"2018\"\n",
    "ds = \"QCD_HT1000to1500_TuneCP5_PSWeights_13TeV-madgraph-pythia8\"\n",
    "events = NanoEventsFactory.from_root(\n",
    "    \"root://cmsxrootd.fnal.gov//store/mc/RunIISummer20UL18NanoAODv9/QCD_HT1000to1500_TuneCP5_PSWeights_13TeV-madgraph-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v16_L1v1-v1/2540000/F1F8548E-48B8-804D-B5E2-3CC8501F39BE.root\",\n",
    "    metadata={\"dataset\": ds},\n",
    "    entry_stop=100000,\n",
    ").events()\n",
    "proc = TriggerProcessor(year=year)\n",
    "out = proc.process(events)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-deficit",
   "metadata": {},
   "source": [
    "# Run on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99dd626-2f70-49be-b8cc-06219f67aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_input = f\"../datasets/infiles/{year}/{year}_trigger.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "environmental-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LPCCondorCluster(ship_env=True)\n",
    "cluster.adapt(minimum=0, maximum=100)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "functioning-credits",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-09e840a5-9e2c-11ef-8025-001a4a90b0d4</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> lpcjobqueue.LPCCondorCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://131.225.191.73:8787/status\" target=\"_blank\">http://131.225.191.73:8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"http://131.225.191.73:8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LPCCondorCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">e017b2e2</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://131.225.191.73:8787/status\" target=\"_blank\">http://131.225.191.73:8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-0129281b-3ad4-4f96-98d2-43c546d48e28</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://131.225.191.73:10032\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://131.225.191.73:8787/status\" target=\"_blank\">http://131.225.191.73:8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> 2 minutes ago\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://131.225.191.73:10032' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decent-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166054.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sqlalchemy/orm/__pycache__/events.cpython-310.pyc; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6054/0/cluster73166054.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sqlalchemy/engine/characteristics.py\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166055.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/edit/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6055/0/cluster73166055.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/components/requirejs/require.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 35.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166056.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.8/site-packages/jupyterlab/labextensions.py; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6056/0/cluster73166056.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/trio/_tests/__pycache__/module_with_deprecations.cpython-310.pyc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  1min 32.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166067.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6067/0/cluster73166067.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  1min 37.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166068.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/share/jupyter/lab/static/3de784d07b9fa8f104c1.woff; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6068/0/cluster73166068.proc0.subproc0.tmp/.env/lib/python3.8/site-packages/qtpy/tests/__pycache__/test_qtmacextras.cpython-38.pyc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  1min 42.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166069.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6069/0/cluster73166069.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 1% Completed |  1min 52.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166071.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6071/0/cluster73166071.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 1% Completed |  1min 55.7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166072.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sphinx_rtd_theme/static/css/fonts/lato-normal-italic.woff; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6072/0/cluster73166072.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinx/util/__pycache__/__init__.cpython-310.pyc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 2% Completed |  1min 58.7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166073.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/notebook/static/be0a084962d8066884f7.svg; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6073/0/cluster73166073.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/notebook/static/8548.bundle.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 2% Completed |  2min  4.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166074.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6074/0/cluster73166074.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 3% Completed |  2min 12.5s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166076.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/trio/_tests/test_highlevel_ssl_helpers.py; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6076/0/cluster73166076.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sqlalchemy/testing/fixtures/__pycache__/orm.cpython-310.pyc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 3% Completed |  2min 15.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166077.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/notebook/static/5125.bundle.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6077/0/cluster73166077.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/tree/js/main.min.js.map\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##                                      ] | 6% Completed |  2min 34.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166081.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6081/0/cluster73166081.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##                                      ] | 7% Completed |  2min 39.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166082.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6082/0/cluster73166082.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###                                     ] | 9% Completed |  2min 43.8s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166083.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6083/0/cluster73166083.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####                                   ] | 14% Completed |  2min 58.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166086.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6086/0/cluster73166086.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######                                  ] | 16% Completed |  3min  2.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166087.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6087/0/cluster73166087.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sqlalchemy/orm/__pycache__/_typing.cpython-310.pyc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#######                                 ] | 18% Completed |  3min  6.7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166088.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6088/0/cluster73166088.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########                                ] | 22% Completed |  3min 16.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166090.0: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#########                               ] | 23% Completed |  3min 19.7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166091.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sqlalchemy/cyextension/collections.cpython-310-x86_64-linux-gnu.so; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6091/0/cluster73166091.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinx_rtd_theme/static/js/html5shiv.min.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##########                              ] | 26% Completed |  3min 26.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166093.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/bin/python3; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6093/0/cluster73166093.proc0.subproc0.tmp/.env/bin/python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[############                            ] | 32% Completed |  3min 38.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166096.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/notebook/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6096/0/cluster73166096.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/edit/js/main.min.js.map\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[################                        ] | 42% Completed |  3min 57.6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166100.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6100/0/cluster73166100.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##################                      ] | 45% Completed |  4min  2.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166101.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6101/0/cluster73166101.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#######################                 ] | 58% Completed |  4min 40.8s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166109.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sqlalchemy/cyextension/resultproxy.cpython-310-x86_64-linux-gnu.so; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6109/0/cluster73166109.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinxcontrib/devhelp/locales/hu/LC_MESSAGES/sphinxcontrib.devhelp.po\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#######################                 ] | 59% Completed |  4min 44.3s\u001b[2K\r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "file not found ([ERROR] Server responded with an error: [3011] No servers are available to read the file.\n)\n\n    'root://cmsxrootd.fnal.gov//store/test/xrootd/T1_US_FNAL/store/data/Run2018D/SingleMuon/NANOAOD/UL2018_MiniAODv2_NanoAODv9-v1/280000/BDAFAB90-144A-154F-8476-6F314036373F.root'\n\nFiles may be specified as:\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\n         other than Windows drive letter or URL schema.\n         Examples: \"rel/file.root\", \"C:\\abs\\file.root\", \"http://where/what.root\"\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\n         Example: \"rel/file.root:tdirectory/ttree\"\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\n         object-within-ROOT path), regardless of whether there are any colons.\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\n\nFunctions that accept many files (uproot.iterate, etc.) also allow:\n   * glob syntax in str/bytes and pathlib.Path.\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\n   * already-open TTree objects.\n   * iterables of the above.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_uproot_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdask_executor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"align_clusters\": True,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mschema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNanoAODSchema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msavemetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/__init__.py:104\u001b[0m, in \u001b[0;36m_run_x_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache, dynamic_chunksize, format)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# make Runner instance, assume other args are for _work_function & co.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m run \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m     95\u001b[0m     executor\u001b[38;5;241m=\u001b[39mexecutor,\n\u001b[1;32m     96\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexecutor_args,\n\u001b[1;32m    102\u001b[0m )\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1700\u001b[0m, in \u001b[0;36mRunner.__call__\u001b[0;34m(self, fileset, treename, processor_instance)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1681\u001b[0m     fileset: Dict,\n\u001b[1;32m   1682\u001b[0m     treename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1683\u001b[0m     processor_instance: ProcessorABC,\n\u001b[1;32m   1684\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Accumulatable:\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the processor_instance on a given fileset\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \n\u001b[1;32m   1687\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m            An instance of a class deriving from ProcessorABC\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     wrapped_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dataframes:\n\u001b[1;32m   1702\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_out  \u001b[38;5;66;03m# not wrapped anymore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1782\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, fileset, processor_instance, treename)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m fileset\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1782\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1785\u001b[0m     pi_to_send \u001b[38;5;241m=\u001b[39m processor_instance\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1734\u001b[0m, in \u001b[0;36mRunner.preprocess\u001b[0;34m(self, fileset, treename)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filemeta \u001b[38;5;129;01min\u001b[39;00m fileset:\n\u001b[1;32m   1732\u001b[0m     filemeta\u001b[38;5;241m.\u001b[39mmaybe_populate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_cache)\n\u001b[0;32m-> 1734\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_fileset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1735\u001b[0m fileset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_badfiles(fileset)\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;66;03m# reverse fileset list to match the order of files as presented in version\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;66;03m# v0.7.4. This fixes tests using maxchunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1461\u001b[0m, in \u001b[0;36mRunner._preprocess_fileset\u001b[0;34m(self, fileset)\u001b[0m\n\u001b[1;32m   1454\u001b[0m pre_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_executor\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpre_arg_override)\n\u001b[1;32m   1455\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_retries,\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipbadfiles,\n\u001b[1;32m   1459\u001b[0m     partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_fetcher, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxrootdtimeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malign_clusters),\n\u001b[1;32m   1460\u001b[0m )\n\u001b[0;32m-> 1461\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpre_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_get\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m out:\n\u001b[1;32m   1463\u001b[0m     item \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:972\u001b[0m, in \u001b[0;36mDaskExecutor.__call__\u001b[0;34m(self, items, function, accumulator)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;66;03m# FIXME: fancy widget doesn't appear, have to live with boring pbar\u001b[39;00m\n\u001b[1;32m    968\u001b[0m         progress(work, multi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, notebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    970\u001b[0m         accumulate(\n\u001b[1;32m    971\u001b[0m             [\n\u001b[0;32m--> 972\u001b[0m                 \u001b[43mwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    974\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m _decompress(work\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m    975\u001b[0m             ],\n\u001b[1;32m    976\u001b[0m             accumulator,\n\u001b[1;32m    977\u001b[0m         ),\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m KilledWorker \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    981\u001b[0m     baditem \u001b[38;5;241m=\u001b[39m key_to_item[ex\u001b[38;5;241m.\u001b[39mtask]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/client.py:287\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    286\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1367\u001b[0m, in \u001b[0;36mautomatic_retries\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m skipbadfiles\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuth failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chain)\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m retries \u001b[38;5;241m==\u001b[39m retry_count\n\u001b[1;32m   1366\u001b[0m     ):\n\u001b[0;32m-> 1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1368\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (retry_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1369\u001b[0m retry_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1336\u001b[0m, in \u001b[0;36mautomatic_retries\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m retries:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1336\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;66;03m# catch xrootd errors and optionally skip\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# or retry to read the file\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1417\u001b[0m, in \u001b[0;36mmetadata_fetcher\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata_fetcher\u001b[39m(\n\u001b[1;32m   1415\u001b[0m     xrootdtimeout: \u001b[38;5;28mint\u001b[39m, align_clusters: \u001b[38;5;28mbool\u001b[39m, item: FileMeta\n\u001b[1;32m   1416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Accumulatable:\n\u001b[0;32m-> 1417\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m uproot\u001b[38;5;241m.\u001b[39mopen({item\u001b[38;5;241m.\u001b[39mfilename: \u001b[38;5;28;01mNone\u001b[39;00m}, timeout\u001b[38;5;241m=\u001b[39mxrootdtimeout) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1419\u001b[0m             tree \u001b[38;5;241m=\u001b[39m file[item\u001b[38;5;241m.\u001b[39mtreename]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/reading.py:141\u001b[0m, in \u001b[0;36mopen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m uproot\u001b[38;5;241m.\u001b[39m_util\u001b[38;5;241m.\u001b[39misstr(file_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m ):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a string, pathlib.Path, an object with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m methods, or a length-1 dict of \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mfile_path: object_path}}, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mrepr\u001b[39m(path))\n\u001b[1;32m    139\u001b[0m     )\n\u001b[0;32m--> 141\u001b[0m file \u001b[38;5;241m=\u001b[39m ReadOnlyFile(\n\u001b[1;32m    142\u001b[0m     file_path,\n\u001b[1;32m    143\u001b[0m     object_cache\u001b[38;5;241m=\u001b[39mobject_cache,\n\u001b[1;32m    144\u001b[0m     array_cache\u001b[38;5;241m=\u001b[39marray_cache,\n\u001b[1;32m    145\u001b[0m     custom_classes\u001b[38;5;241m=\u001b[39mcustom_classes,\n\u001b[1;32m    146\u001b[0m     decompression_executor\u001b[38;5;241m=\u001b[39mdecompression_executor,\n\u001b[1;32m    147\u001b[0m     interpretation_executor\u001b[38;5;241m=\u001b[39minterpretation_executor,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,  \u001b[38;5;66;03m# NOTE: a comma after **options breaks Python 2\u001b[39;00m\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m object_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mroot_directory\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/reading.py:580\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_before_create_source()\n\u001b[1;32m    577\u001b[0m Source, file_path \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39m_util\u001b[38;5;241m.\u001b[39mfile_path_to_source_class(\n\u001b[1;32m    578\u001b[0m     file_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source \u001b[38;5;241m=\u001b[39m Source(\n\u001b[1;32m    581\u001b[0m     file_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options  \u001b[38;5;66;03m# NOTE: a comma after **options breaks Python 2\u001b[39;00m\n\u001b[1;32m    582\u001b[0m )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_before_get_chunks()\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m _file_header_fields_big\u001b[38;5;241m.\u001b[39msize:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/source/xrootd.py:272\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/source/xrootd.py:275\u001b[0m, in \u001b[0;36m_open\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource \u001b[38;5;241m=\u001b[39m XRootDResource(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# this ThreadPool does not need a resource, it's only used to submit\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# futures that wait for chunks that have been split to merge them.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mResourceThreadPoolExecutor(\n\u001b[1;32m    280\u001b[0m         [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers)]\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/source/xrootd.py:83\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m=\u001b[39m timeout\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/source/xrootd.py:92\u001b[0m, in \u001b[0;36m_open\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m status, dummy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xrd_timeout())\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xrd_error(status)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/uproot/source/xrootd.py:115\u001b[0m, in \u001b[0;36m_xrd_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;66;03m# https://github.com/xrootd/xrootd/blob/8e91462e76ab969720b40fc324714b84e0b4bd42/src/XrdCl/XrdClStatus.hh#L47-L103\u001b[39;00m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;66;03m# https://github.com/xrootd/xrootd/blob/250eced4d3787c2ac5be2c8c922134153bbf7f08/src/XrdCl/XrdClStatus.cc#L34-L74\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m status\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m101\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m status\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m304\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m status\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m uproot\u001b[38;5;241m.\u001b[39m_util\u001b[38;5;241m.\u001b[39m_file_not_found(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path, status\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"XRootD error: {}\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03min file {}\"\"\"\u001b[39;00m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    121\u001b[0m                     status\u001b[38;5;241m.\u001b[39mmessage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path\n\u001b[1;32m    122\u001b[0m                 )\n\u001b[1;32m    123\u001b[0m             )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: file not found ([ERROR] Server responded with an error: [3011] No servers are available to read the file.\n)\n\n    'root://cmsxrootd.fnal.gov//store/test/xrootd/T1_US_FNAL/store/data/Run2018D/SingleMuon/NANOAOD/UL2018_MiniAODv2_NanoAODv9-v1/280000/BDAFAB90-144A-154F-8476-6F314036373F.root'\n\nFiles may be specified as:\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\n         other than Windows drive letter or URL schema.\n         Examples: \"rel/file.root\", \"C:\\abs\\file.root\", \"http://where/what.root\"\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\n         Example: \"rel/file.root:tdirectory/ttree\"\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\n         object-within-ROOT path), regardless of whether there are any colons.\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\n\nFunctions that accept many files (uproot.iterate, etc.) also allow:\n   * glob syntax in str/bytes and pathlib.Path.\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\n   * already-open TTree objects.\n   * iterables of the above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166114.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/components/sanitizer/index.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6114/0/cluster73166114.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/components/font-awesome/fonts/fontawesome-webfont.svg\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166123.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6123/0/cluster73166123.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10478' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10491' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10492' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10490' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10505' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10518' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10480' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10481' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10482' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10495' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10465' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10483' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10496' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10509' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10497' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10485' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10510' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10523' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10486' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10499' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10500' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10532' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10463' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10476' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10464' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10477' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10502' coro=<_wrap_awaitable() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:118> exception=AssertionError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 119, in _wrap_awaitable\n",
      "    return await aw\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166127.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6127/0/cluster73166127.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "2024-11-08 23:55:32,456 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f8d1a01ee50>>, <Task finished name='Task-10460' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:341> exception=AssertionError()>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tornado/ioloop.py\", line 762, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 382, in _correct_state_internal\n",
      "    await w  # for tornado gen.coroutine support\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f8d1a01ee50>>, <Task finished name='Task-10460' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py:341> exception=AssertionError()>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tornado/ioloop.py\", line 762, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 382, in _correct_state_internal\n",
      "    await w  # for tornado gen.coroutine support\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 69, in _\n",
      "    assert self.status == Status.running\n",
      "AssertionError\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166128.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sqlalchemy/cyextension/processors.cpython-310-x86_64-linux-gnu.so; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6128/0/cluster73166128.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinxcontrib/applehelp/locales/fa/LC_MESSAGES/sphinxcontrib.applehelp.po\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166129.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/notebook/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6129/0/cluster73166129.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/edit/js/main.min.js\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166138.0: \n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166139.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/notebook/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6139/0/cluster73166139.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/edit/js/main.min.js\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166150.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6150/0/cluster73166150.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166159.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6159/0/cluster73166159.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166160.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6160/0/cluster73166160.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166161.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6161/0/cluster73166161.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166162.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.8/site-packages/jupyterlab/staging/yarn.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6162/0/cluster73166162.proc0.subproc0.tmp/.env/lib/python3.8/site-packages/h11-0.14.0.dist-info/METADATA\n",
      "Failed to commit and disconnect from queue.\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166168.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sphinx/locale/ko/LC_MESSAGES/sphinx.mo; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6168/0/cluster73166168.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinx/locale/en_HK/LC_MESSAGES/sphinx.po\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166172.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6172/0/cluster73166172.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166173.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6173/0/cluster73166173.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166174.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6174/0/cluster73166174.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166175.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6175/0/cluster73166175.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166176.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6176/0/cluster73166176.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166177.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6177/0/cluster73166177.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166178.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sqlalchemy/cyextension/processors.cpython-310-x86_64-linux-gnu.so; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6178/0/cluster73166178.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinxcontrib/serializinghtml/locales/sr_RS/LC_MESSAGES/sphinxcontrib.serializinghtml.po\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166179.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6179/0/cluster73166179.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166180.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6180/0/cluster73166180.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166181.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/sphinx_rtd_theme/static/css/fonts/lato-normal-italic.woff; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6181/0/cluster73166181.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/sphinx_rtd_theme/locale/da/LC_MESSAGES/sphinx.po\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166182.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6182/0/cluster73166182.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166185.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6185/0/cluster73166185.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166186.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>; SCHEDD at 131.225.189.168 failed to create directory /storage/local/data1/condor/spool/6186/0/cluster73166186.proc0.subproc0.tmp/.env/share/man/man1: No such file or directory (errno 2)\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166187.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/share/jupyter/lab/static/4986.a497cdda4b7152902568.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6187/0/cluster73166187.proc0.subproc0.tmp/.env/lib/python3.8/site-packages/qtpy/QtWidgets.py\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166188.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/notebook/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6188/0/cluster73166188.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/edit/js/main.min.js\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166189.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/tree/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6189/0/cluster73166189.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/notebook/js/main.min.js.map\n",
      "DCSchedd::spoolJobFiles:7002:File transfer failed for target job 73166190.0: TOOL at 131.225.191.73 failed to send file(s) to <131.225.189.168:9618>: error sending /uscmst1b_scratch/lpc1/3DayLifetime/dhoang/tmpw37jyfkm/.env/lib/python3.10/site-packages/nbclassic/static/edit/js/main.min.js; SCHEDD at 131.225.189.168 - |Error: receiving file /storage/local/data1/condor/spool/6190/0/cluster73166190.proc0.subproc0.tmp/.env/lib/python3.10/site-packages/nbclassic/static/components/moment/min/locales.js\n"
     ]
    }
   ],
   "source": [
    "out, metrics = processor.run_uproot_job(\n",
    "    json_input,\n",
    "    \"Events\",\n",
    "    proc,\n",
    "    processor.dask_executor,\n",
    "    {\n",
    "        \"client\": client,\n",
    "        # \"align_clusters\": True,\n",
    "        \"retries\": 3,\n",
    "        \"schema\": processor.NanoAODSchema,\n",
    "        \"savemetrics\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coffea.util\n",
    "coffea.util.save(out, f\"triggerstudy_newmu_{year}.coffea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-thomson",
   "metadata": {},
   "source": [
    "# Analyze original set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import pandas as pd\n",
    "import coffea.util\n",
    "import coffea.hist\n",
    "import re\n",
    "\n",
    "plt.style.use(mplhep.style.CMS)\n",
    "year = \"2018\"\n",
    "out = coffea.util.load(f\"triggerstudy_newmu_{year}.coffea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    "    .integrate(\"era\", \"MC\")\n",
    "    .sum(\"ddb\")\n",
    "    .integrate(\"trigger\", \"none\")\n",
    "    .to_hist()\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "for cat in x.axes[0]:\n",
    "    mplhep.histplot(x[cat, :], ax=ax, label=cat)\n",
    "    \n",
    "ax.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\")  # puts all QCD into \"MC\" era\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    "    .integrate(\"pt\", slice(450, None))\n",
    "    .sum(\"ddb\")\n",
    ")\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        x.label: x.values().values()\n",
    "    },\n",
    "    index=pd.MultiIndex.from_tuples(x.values().keys(), names=[ax.name for ax in x.axes()])\n",
    ").unstack()\n",
    "efftable = df / df.loc[\"none\"]\n",
    "efftable.columns = efftable.columns.droplevel(0)\n",
    "with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "    print(efftable.to_latex())\n",
    "    display(efftable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ptproj = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\", \"SingleMuon\")\n",
    "    .sum(\"era\", \"ddb\")\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    ")\n",
    "denom = ptproj.integrate(\"trigger\", \"none\")\n",
    "for tname in ptproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"none\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=ptproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "    \n",
    "coffea.hist.plotratio(\n",
    "    num=(\n",
    "        out[\"trigger_inclusive\"]\n",
    "        .integrate(\"dataset\", \"SingleMuon\")\n",
    "        .sum(\"era\", \"ddb\")\n",
    "        .integrate(\"msd\", slice(40., None))\n",
    "        .integrate(\"trigger\", \"all\")\n",
    "    ),\n",
    "    denom=denom,\n",
    "    error_opts={'linestyle': 'dotted'},\n",
    "    label=\"Soup\",\n",
    "    ax=ax,\n",
    "    clear=False,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Efficiency\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.set_xlim(200, None)\n",
    "ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=\"Triggers ($\\mu$ ref, jet $m_{SD}\\geq 40$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=True, year=year)\n",
    "fig.savefig(f\"exclusive_efficiency_data_pt_{year}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ptproj = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\", \"QCD*\")\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    "    .integrate(\"era\", \"MC\")\n",
    "    .sum(\"ddb\")\n",
    ")\n",
    "denom = ptproj.integrate(\"trigger\", \"none\")\n",
    "for tname in ptproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"none\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=ptproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "    \n",
    "coffea.hist.plotratio(\n",
    "    num=(\n",
    "        out[\"trigger_inclusive\"]\n",
    "        .integrate(\"dataset\", \"QCD*\")\n",
    "        .sum(\"era\", \"ddb\")\n",
    "        .integrate(\"msd\", slice(40., None))\n",
    "        .integrate(\"trigger\", \"all\")\n",
    "    ),\n",
    "    denom=denom,\n",
    "    error_opts={'linestyle': 'dotted'},\n",
    "    label=\"Soup\",\n",
    "    ax=ax,\n",
    "    clear=False,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Efficiency\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.set_xlim(200, None)\n",
    "ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=\"Triggers (QCD MC, jet $m_{SD} \\geq 40$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=False, year=year)\n",
    "fig.savefig(f\"exclusive_efficiency_mc_pt_{year}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "msdproj = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\", \"SingleMuon\")\n",
    "    .sum(\"era\", \"ddb\")\n",
    "    .integrate(\"pt\", slice(450., None))\n",
    ")\n",
    "denom = msdproj.integrate(\"trigger\", \"none\")\n",
    "for tname in msdproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"none\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=msdproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "\n",
    "coffea.hist.plotratio(\n",
    "    num=(\n",
    "        out[\"trigger_inclusive\"]\n",
    "        .integrate(\"dataset\", \"SingleMuon\")\n",
    "        .sum(\"era\", \"ddb\")\n",
    "        .integrate(\"pt\", slice(450., None))\n",
    "        .integrate(\"trigger\", \"all\")\n",
    "    ),\n",
    "    denom=denom,\n",
    "    error_opts={'linestyle': 'dotted'},\n",
    "    label=\"Soup\",\n",
    "    ax=ax,\n",
    "    clear=False,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Efficiency\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=\"Triggers ($\\mu$ ref, jet $p_T \\geq 450$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=True, year=year)\n",
    "fig.savefig(f\"exclusive_efficiency_data_msd_{year}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "msdproj = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\", \"QCD*\")\n",
    "    .integrate(\"era\", \"MC\")\n",
    "    .sum(\"ddb\")\n",
    "    .integrate(\"pt\", slice(450., None))\n",
    ")\n",
    "denom = msdproj.integrate(\"trigger\", \"none\")\n",
    "for tname in msdproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"none\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=msdproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "    \n",
    "coffea.hist.plotratio(\n",
    "    num=(\n",
    "        out[\"trigger_inclusive\"]\n",
    "        .integrate(\"dataset\", \"QCD*\")\n",
    "        .integrate(\"era\", \"MC\")\n",
    "        .sum(\"ddb\")\n",
    "        .integrate(\"pt\", slice(450., None))\n",
    "        .integrate(\"trigger\", \"all\")\n",
    "    ),\n",
    "    denom=denom,\n",
    "    error_opts={'linestyle': 'dotted'},\n",
    "    label=\"Soup\",\n",
    "    ax=ax,\n",
    "    clear=False,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Efficiency\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=\"Triggers (QCD MC, jet $p_T \\geq 450$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=True, year=year)\n",
    "fig.savefig(f\"exclusive_efficiency_mc_msd_{year}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.hist import clopper_pearson_interval\n",
    "num = (\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"trigger\", \"all\")\n",
    "    .integrate(\"dataset\")\n",
    "    .sum(\"ddb\")\n",
    "    .rebin(\"pt\", 2)\n",
    "    .rebin(\"msd\", 2)\n",
    ")\n",
    "denom = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"trigger\", \"none\")\n",
    "    .integrate(\"dataset\")\n",
    "    .sum(\"ddb\")\n",
    "    .rebin(\"pt\", 2)\n",
    "    .rebin(\"msd\", 2)\n",
    ")\n",
    "alldata = re.compile(\"(Run)?201[678]\")\n",
    "rdata = (\n",
    "    num.integrate(\"era\", alldata).to_hist().view() \n",
    "    / np.maximum(1, denom.integrate(\"era\", alldata).to_hist().view())\n",
    ")\n",
    "rdata_unc = clopper_pearson_interval(\n",
    "    num.integrate(\"era\", alldata).to_hist().view(),\n",
    "    denom.integrate(\"era\", alldata).to_hist().view(),\n",
    ")\n",
    "\n",
    "rmc = (\n",
    "    num.integrate(\"era\", \"MC\").to_hist().view()\n",
    "    / np.maximum(1, denom.integrate(\"era\", \"MC\").to_hist().view())\n",
    ")\n",
    "rmc_unc = clopper_pearson_interval(\n",
    "    num.integrate(\"era\", \"MC\").to_hist().view(),\n",
    "    denom.integrate(\"era\", \"MC\").to_hist().view(),\n",
    ")\n",
    "\n",
    "scalefactor = rdata / np.where(rmc == 0, 1., rmc)\n",
    "scalefactor_up = rdata_unc[1] / np.where(rmc_unc[0] == 0, np.inf, rmc_unc[0])\n",
    "scalefactor_dn = rdata_unc[0] / np.where(rmc_unc[1] == 0, 1., rmc_unc[1])\n",
    "x = num.integrate(\"era\").to_hist()\n",
    "y = x.copy()\n",
    "x[...] = (scalefactor_up - scalefactor_dn) / 2 / scalefactor\n",
    "y[...] = scalefactor\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "artist = mplhep.hist2dplot(y.project(1, 0), ax=ax, cmin=0, cmax=2)\n",
    "artist.cbar.set_label(\"Soup efficiency scale factor\")\n",
    "mplhep.cms.label(ax=ax, data=True, year=year)\n",
    "fig.savefig(f\"scalefactor_soup_{year}.pdf\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "artist = mplhep.hist2dplot(x.project(1, 0), ax=ax, cmin=0, cmax=0.1)\n",
    "artist.cbar.set_label(\"Soup efficiency scale factor uncertainty\")\n",
    "mplhep.cms.label(ax=ax, data=True, year=year)\n",
    "fig.savefig(f\"scalefactor_soup_unc_{year}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79787c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from correctionlib import schemav2\n",
    "\n",
    "soup_names = [x.name for x in out[\"trigger_inclusive\"].identifiers(\"trigger\") if x.name != \"all\"]\n",
    "\n",
    "\n",
    "def multibinning(sf):\n",
    "    return schemav2.MultiBinning(\n",
    "        nodetype=\"multibinning\",\n",
    "        inputs=[\"pt\", \"msd\"],\n",
    "        edges=[list(ax.edges) for ax in x.axes],\n",
    "        content=list(sf.flatten()),\n",
    "        flow=\"clamp\",\n",
    "    )\n",
    "\n",
    "\n",
    "corr = schemav2.Correction(\n",
    "    name=f\"fatjet_triggerSF{year}\",\n",
    "    description=\"Year-averaged data-to-simulation correction for trigger soup: \" + \",\".join(soup_names),\n",
    "    version=1,\n",
    "    inputs=[\n",
    "        schemav2.Variable(\n",
    "            name=\"systematic\",\n",
    "            type=\"string\",\n",
    "            description=\"Systematic variation\",\n",
    "        ),\n",
    "        schemav2.Variable(\n",
    "            name=\"pt\",\n",
    "            type=\"real\",\n",
    "            description=\"Jet transverse momentum (NanoAODv7 nominal value)\",\n",
    "        ),\n",
    "        schemav2.Variable(\n",
    "            name=\"msd\",\n",
    "            type=\"real\",\n",
    "            description=\"Jet softdrop mass (NanoAODv7 nominal value)\",\n",
    "        ),\n",
    "    ],\n",
    "    output=schemav2.Variable(name=\"weight\", type=\"real\", description=\"Event weight to correct MC to data\"),\n",
    "    data=schemav2.Category(\n",
    "        nodetype=\"category\",\n",
    "        input=\"systematic\",\n",
    "        content=[\n",
    "            {\"key\": \"nominal\", \"value\": multibinning(scalefactor)},\n",
    "            {\"key\": \"stat_up\", \"value\": multibinning(scalefactor_up)},\n",
    "            {\"key\": \"stat_dn\", \"value\": multibinning(scalefactor_dn)},\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "cset = schemav2.CorrectionSet(schema_version=2, corrections=[corr])\n",
    "with open(f\"fatjet_triggerSF{year}.json\", \"w\") as fout:\n",
    "    fout.write(cset.json(exclude_unset=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.lookup_tools.dense_lookup import dense_lookup\n",
    "\n",
    "lookups = {\n",
    "    year + \"_jettrigger\": dense_lookup(scalefactor, [ax.edges for ax in x.axes]),\n",
    "    year + \"_jettrigger_up\": dense_lookup(scalefactor_up, [ax.edges for ax in x.axes]),\n",
    "    year + \"_jettrigger_down\": dense_lookup(scalefactor_dn, [ax.edges for ax in x.axes]),\n",
    "    year + \"_triggers\": TriggerProcessor()._triggers[year],\n",
    "}\n",
    "coffea.util.save(lookups, f\"jettrigger_sf{year}.coffea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "msdproj = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\", \"QCD*\")\n",
    "    .integrate(\"era\", \"MC\")\n",
    "    .integrate(\"pt\", slice(450., None))\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    ")\n",
    "denom = msdproj.integrate(\"trigger\", \"none\")\n",
    "for tname in msdproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"none\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=msdproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "    \n",
    "coffea.hist.plotratio(\n",
    "    num=(\n",
    "        out[\"trigger_inclusive\"]\n",
    "        .integrate(\"dataset\", \"QCD*\")\n",
    "        .integrate(\"era\", \"MC\")\n",
    "        .integrate(\"pt\", slice(450., None))\n",
    "        .integrate(\"msd\", slice(40, None))\n",
    "        .integrate(\"trigger\", \"all\")\n",
    "    ),\n",
    "    denom=denom,\n",
    "    error_opts={'linestyle': 'dotted'},\n",
    "    label=\"Soup\",\n",
    "    ax=ax,\n",
    "    clear=False,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Efficiency\")\n",
    "ax.set_ylim(0.5, 1.5)\n",
    "ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=\"Triggers (QCD MC, jet $p_T \\geq 450$, $m_{SD} \\geq 40$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=False, year=year)\n",
    "fig.savefig(f\"exclusive_efficiency_mc_ddb_{year}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-chaos",
   "metadata": {},
   "source": [
    "# Other stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ptproj = (\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    "    .integrate(\"era\", \"2017*\")\n",
    ")\n",
    "denom = ptproj.integrate(\"trigger\", \"all\")\n",
    "for tname in ptproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"all\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=ptproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"N(soup \\ trigger) / N(soup)\")\n",
    "ax.set_ylim(0.8, 1.01)\n",
    "ax.set_xlim(400, 700)\n",
    "# ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=\"N-1 efficiency ($\\mu$ ref, jet $m_{SD}\\geq40$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=True, year=2017)\n",
    "fig.savefig(\"inclusive_efficiency_data_pt_2017.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    "    .integrate(\"era\", \"2017*\")\n",
    ")[:, 450.:].to_hist()\n",
    "r = (h.view() / h[\"all\", :].view())\n",
    "r = np.nan_to_num(r, nan=1.).min(axis=1)\n",
    "pd.DataFrame({\"avg eff\": r}, index=h.axes[0]).sort_values(\"avg eff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ptproj = (\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .sum(\"msd\")\n",
    "    .integrate(\"era\", \"MC\")\n",
    ")\n",
    "denom = ptproj.integrate(\"trigger\", \"all\")\n",
    "for tname in ptproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"all\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=ptproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"N(soup \\ trigger) / N(soup)\")\n",
    "ax.set_ylim(0.8, 1.01)\n",
    "ax.set_xlim(400, 700)\n",
    "# ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "mplhep.cms.label(ax=ax, data=False, year=2017)\n",
    "ax.legend(title=\"N-1 efficiency (QCD MC)\", fontsize=14)\n",
    "fig.savefig(\"inclusive_efficiency_mc_pt_2017.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "msdproj = (\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .integrate(\"pt\", slice(450, None))\n",
    "    .integrate(\"era\", \"2017*\")\n",
    ")\n",
    "denom = msdproj.integrate(\"trigger\", \"all\")\n",
    "for tname in msdproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"all\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=msdproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"N(soup \\ trigger) / N(soup)\")\n",
    "ax.set_ylim(0.9, 1.01)\n",
    "mplhep.cms.label(ax=ax, data=True, year=2017)\n",
    "ax.legend(title=\"N-1 efficiency ($\\mu$ ref, jet $p_T \\geq 450$)\", fontsize=14)\n",
    "fig.savefig(\"inclusive_efficiency_data_msd_2017.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "msdproj = (\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .integrate(\"pt\", slice(450, None))\n",
    "    .integrate(\"era\", \"MC\")\n",
    ")\n",
    "denom = msdproj.integrate(\"trigger\", \"all\")\n",
    "for tname in msdproj.identifiers(\"trigger\"):\n",
    "    if tname.name == \"all\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=msdproj.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"N(soup \\ trigger) / N(soup)\")\n",
    "mplhep.cms.label(ax=ax, data=False, year=2017)\n",
    "ax.legend(title=\"N-1 efficiency (QCD MC, jet $p_T \\geq 450$)\", fontsize=14)\n",
    "fig.savefig(\"inclusive_efficiency_mc_msd_2017.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-remainder",
   "metadata": {},
   "source": [
    "# Scale factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = coffea.util.load(\"triggerstudy2017.coffea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.lookup_tools.dense_lookup import dense_lookup\n",
    "\n",
    "lookups = {\n",
    "    \"2017_jettrigger\": dense_lookup(scalefactor, [ax.edges for ax in x.axes]),\n",
    "    \"2017_jettrigger_up\": dense_lookup(scalefactor_up, [ax.edges for ax in x.axes]),\n",
    "    \"2017_jettrigger_down\": dense_lookup(scalefactor_dn, [ax.edges for ax in x.axes]),\n",
    "    \"2017_triggers\": TriggerProcessor()._triggers[\"2017\"],\n",
    "}\n",
    "coffea.util.save(lookups, \"jettrigger_sf.coffea\")\n",
    "lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "oldsf = uproot.open(\"https://github.com/nsmith-/coffeandbacon/raw/master/analysis/correction_files/TrigEff_2017BtoF_noPS_Feb21.root\")\n",
    "oldsf = oldsf[\"h_numer;1\"].to_hist() / oldsf[\"h_denom;1\"].to_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "artist = mplhep.hist2dplot(oldsf, ax=ax, cmin=0, cmax=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "artist = mplhep.hist2dplot(rdata.project(1, 0), ax=ax, cmin=0, cmax=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp2d\n",
    "\n",
    "oldsfp = interp2d(*np.meshgrid(oldsf.axes[1].centers, oldsf.axes[0].centers), np.nan_to_num(oldsf.view(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldsfrebin = oldsfp(rdata.axes[0].centers, rdata.axes[1].centers)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "artist = mplhep.hist2dplot(rdata.project(1, 0) / oldsfrebin, ax=ax, cmin=0, cmax=2)\n",
    "artist.cbar.set_label(\"Soup data efficiency / old measurement\")\n",
    "fig.savefig(\"efficiency_soup_2017_vs_old.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-savage",
   "metadata": {},
   "source": [
    "# Check DDB correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = coffea.util.load(\"triggerstudy2017_ddb.coffea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .sum(\"era\")\n",
    "    .integrate(\"pt\", slice(450, None))\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    ")\n",
    "denom = (\n",
    "    out[\"trigger_exclusive\"]\n",
    "    .integrate(\"trigger\", \"none\")\n",
    "    .integrate(\"dataset\")\n",
    "    .sum(\"era\")\n",
    "    .integrate(\"pt\", slice(450, None))\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for tname in num.identifiers(\"trigger\"):\n",
    "    if tname.name == \"none\":\n",
    "        continue\n",
    "    coffea.hist.plotratio(\n",
    "        num=num.integrate(\"trigger\", tname),\n",
    "        denom=denom,\n",
    "        error_opts={'linestyle': '-'},\n",
    "        label=tname,\n",
    "        ax=ax,\n",
    "        clear=False,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Efficiency\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "#ax.set_xlim(200, None)\n",
    "ax.axhline(y=1, linestyle=\"--\", color=\"gray\")\n",
    "ax.legend(title=r\"Triggers (QCD MC, jet $p_T \\geq 450$, $m_{SD}\\geq 40$)\", fontsize=14)\n",
    "mplhep.cms.label(ax=ax, data=False, year=\"2017\")\n",
    "fig.savefig(\"exlusive_efficiency_mc_ddb_2017.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "coffea.hist.plot1d(\n",
    "    out[\"trigger_inclusive\"]\n",
    "    .integrate(\"dataset\")\n",
    "    .sum(\"era\")\n",
    "    .integrate(\"pt\", slice(450, None))\n",
    "    .integrate(\"msd\", slice(40, None))\n",
    "    .integrate(\"trigger\", \"all\")\n",
    ")\n",
    "coffea.hist.plot1d(num.integrate(\"trigger\", \"AK8PFJet330_PFAK8BTagCSV_p17\"))\n",
    "ax.set_ylim(1e2, 1e8)\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-charm",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
